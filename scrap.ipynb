{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8933d580",
   "metadata": {},
   "outputs": [],
   "source": [
    "import praw\n",
    "import re\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from nltk.corpus import stopwords\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "# ‚úÖ Initialize Reddit API client (Use your own credentials)\n",
    "reddit = praw.Reddit(\n",
    "    client_id=\"QWw_4O3xM-sXF-x42DjITQ\",\n",
    "    client_secret=\"3-kzXcwWZB9nXWc8YPIDjPX9rhOuJA\",\n",
    "    user_agent=\"reddit_comment_scraper by u/Agile-Bandicoot-6731\"\n",
    ")\n",
    "\n",
    "# ‚úÖ Initialize sentiment analyzer\n",
    "# analyzer = SentimentIntensityAnalyzer()\n",
    "\n",
    "# ‚úÖ List of stop words (download NLTK stopwords if not already)\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# ‚úÖ Clean each comment\n",
    "def clean_comment(comment):\n",
    "    comment = comment.lower()  # Convert to lowercase\n",
    "    comment = re.sub(r'http\\S+', '', comment)  # Remove URLs\n",
    "    comment = re.sub(r'u/[a-zA-Z0-9_]+', '', comment)  # Remove mentions\n",
    "    comment = re.sub(r'[^a-zA-Z\\s]', '', comment)  # Remove punctuation\n",
    "    comment = re.sub(r'\\s+', ' ', comment).strip()  # Remove extra spaces\n",
    "    comment = ' '.join([word for word in comment.split() if word not in stop_words])  # Remove stopwords\n",
    "    return comment\n",
    "\n",
    "# ‚úÖ Fetch comments from r/all\n",
    "def fetch_comments_from_r_all(topic, post_limit=100):\n",
    "    comments = []\n",
    "    \n",
    "    print(f\"\\nüîç Searching in r/all for topic: {topic}\")\n",
    "    posts = reddit.subreddit('all').search(topic, limit=post_limit)\n",
    "    \n",
    "    for post in tqdm(posts, desc=f\"Processing r/all posts\"):\n",
    "        try:\n",
    "            post.comments.replace_more(limit=None)  # Fetch all comments\n",
    "            for comment in post.comments.list():\n",
    "                comments.append({\n",
    "                    \"comment\": comment.body,\n",
    "                    \"subreddit\": post.subreddit.display_name  # Store subreddit info\n",
    "                })\n",
    "        except Exception as e:\n",
    "            print(f\"Error fetching comments for a post: {e}\")\n",
    "\n",
    "    # ‚úÖ Remove duplicate comments\n",
    "    return comments\n",
    "\n",
    "# ‚úÖ Main execution\n",
    "if __name__ == \"__main__\":\n",
    "    topic = \"AI in Education\"\n",
    "    print(f\"\\nüöÄ Starting comment scraping for topic: {topic}\")\n",
    "\n",
    "    comments_data = fetch_comments_from_r_all(topic, post_limit=5)\n",
    "    print(f\"\\n‚úÖ Total unique comments fetched: {len(comments_data)}\")\n",
    "    \n",
    "    # ‚úÖ Convert to DataFrame for easy manipulation\n",
    "    df = pd.DataFrame(comments_data)\n",
    "    \n",
    "    # ‚úÖ Save to CSV file\n",
    "    df.to_csv(f'comments_{topic.replace(\" \", \"_\")}_all_subreddits.csv', index=False)\n",
    "    print(f\"\\nüíæ Comments saved to CSV file: comments_{topic.replace(' ', '_')}_all_subreddits.csv\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
